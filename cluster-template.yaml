apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
kind: NutanixCluster
metadata:
  name: "${CLUSTER_NAME}"
  namespace: "${NAMESPACE}"
spec:
  controlPlaneEndpoint:
    host: "${CONTROLPLANE_ENDPOINT_HOST}"
    port: ${CONTROLPLANE_ENDPOINT_PORT}

---
apiVersion: cluster.x-k8s.io/v1beta1
kind: Cluster
metadata:
  name: "${CLUSTER_NAME}"
spec:
  clusterNetwork:
    services:
      cidrBlocks: ["10.96.0.0/12"]
    pods:
      cidrBlocks: ["192.168.0.0/16"]
    serviceDomain: "cluster.local"
  controlPlaneRef:
    apiVersion: controlplane.cluster.x-k8s.io/v1beta1
    kind: KubeadmControlPlane
    name: "${CLUSTER_NAME}-kcp"
  infrastructureRef:
    apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
    kind: NutanixCluster
    name: "${CLUSTER_NAME}"

---
apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
kind: NutanixMachineTemplate
metadata:
  name: "${CLUSTER_NAME}-mt-0"
spec:
  template:
    spec:
      providerID: "nutanix://${CLUSTER_NAME}-m1"
      vcpusPerSocket: 2
      vcpuSockets: 1
      memorySize: 4Gi
      systemDiskSize: "${NUTANIX_SYSTEMDISK_SIZE}"
      image:
        type: name
        name: "${NUTANIX_MACHINE_TEMPLATE_IMAGE_NAME}"
      cluster:
        type: name
        name: "${NUTANIX_PRISM_ELEMENT_CLUSTER_NAME}"
      subnet:
        type: name
        name: "${NUTANIX_SUBNET_NAME}"

---
apiVersion: controlplane.cluster.x-k8s.io/v1beta1
kind: KubeadmControlPlane
metadata:
  name: "${CLUSTER_NAME}-kcp"
spec:
  replicas: 1
  version: ${KUBERNETES_VERSION}
  machineTemplate:
    infrastructureRef:
      apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
      kind: NutanixMachineTemplate
      name: "${CLUSTER_NAME}-mt-0"
  kubeadmConfigSpec:
    clusterConfiguration:
      apiServer:
        certSANs:
          - localhost
          - 127.0.0.1
          - 0.0.0.0
      controllerManager:
        extraArgs:
          enable-hostpath-provisioner: "true"
    initConfiguration:
      nodeRegistration:
        kubeletExtraArgs:
          # We have to pin the cgroupDriver to cgroupfs as kubeadm >=1.21 defaults to systemd
          # kind will implement systemd support in: https://github.com/kubernetes-sigs/kind/issues/1726
          #cgroup-driver: cgroupfs
          eviction-hard: nodefs.available<0%,nodefs.inodesFree<0%,imagefs.available<0%
    users:
      - name: capiuser
        lockPassword: false
        sudo: ALL=(ALL) NOPASSWD:ALL
        sshAuthorizedKeys:
          - ${NUTANIX_SSH_AUTHORIZED_KEY}
    preKubeadmCommands:
      - echo "before kubeadm call" > /var/log/prekubeadm.log
    postKubeadmCommands:
      - echo export KUBECONFIG=/etc/kubernetes/admin.conf >> /root/.bashrc
      - echo "after kubeadm call" > /var/log/postkubeadm.log
    verbosity: 10

---
apiVersion: bootstrap.cluster.x-k8s.io/v1beta1
kind: KubeadmConfigTemplate
metadata:
  name: "${CLUSTER_NAME}-kcfg-0"
spec:
  template:
    spec:
      joinConfiguration:
        nodeRegistration:
          kubeletExtraArgs:
            # We have to pin the cgroupDriver to cgroupfs as kubeadm >=1.21 defaults to systemd
            # kind will implement systemd support in: https://github.com/kubernetes-sigs/kind/issues/1726
            #cgroup-driver: cgroupfs
            eviction-hard: nodefs.available<0%,nodefs.inodesFree<0%,imagefs.available<0%
      users:
        - name: capiuser
          lockPassword: false
          sudo: ALL=(ALL) NOPASSWD:ALL
          sshAuthorizedKeys:
            - ${NUTANIX_SSH_AUTHORIZED_KEY}
      preKubeadmCommands:
        - echo "before kubeadm call" > /var/log/prekubeadm.log
      postKubeadmCommands:
        - echo "after kubeadm call" > /var/log/postkubeadm.log
      verbosity: 10
      #useExperimentalRetryJoin: true
---
apiVersion: cluster.x-k8s.io/v1beta1
kind: MachineDeployment
metadata:
  labels:
    cluster.x-k8s.io/cluster-name: ${CLUSTER_NAME}
  name: "${CLUSTER_NAME}-wmd"
spec:
  clusterName: "${CLUSTER_NAME}"
  replicas: ${WORKER_MACHINE_COUNT}
  selector:
    matchLabels: {}
  template:
    metadata:
      labels:
        cluster.x-k8s.io/cluster-name: "${CLUSTER_NAME}"
    spec:
      bootstrap:
        configRef:
          apiVersion: bootstrap.cluster.x-k8s.io/v1beta1
          kind: KubeadmConfigTemplate
          name: "${CLUSTER_NAME}-kcfg-0"
      clusterName: "${CLUSTER_NAME}"
      infrastructureRef:
        apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
        kind: NutanixMachineTemplate
        name: "${CLUSTER_NAME}-mt-0"
      version: "${KUBERNETES_VERSION}"

---
apiVersion: cluster.x-k8s.io/v1beta1
kind: MachineHealthCheck
metadata:
  name: "${CLUSTER_NAME}-mhc"
spec:
  clusterName: "${CLUSTER_NAME}"
  maxUnhealthy: 40%
  nodeStartupTimeout: 10m
  selector:
    matchLabels:
      cluster.x-k8s.io/cluster-name: "${CLUSTER_NAME}"
  unhealthyConditions:
    - type: Ready
      status: "False"
      timeout: 300s
    - type: Ready
      status: Unknown
      timeout: 300s
    - type: MemoryPressure
      status: "True"
      timeout: 300s
    - type: DiskPressure
      status: "True"
      timeout: 300s
    - type: PIDPressure
      status: "True"
      timeout: 300s
    - type: NetworkUnavailable
      status: "True"
      timeout: 300s
