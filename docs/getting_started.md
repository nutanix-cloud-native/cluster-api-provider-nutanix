# Getting Started

This is a guide on how to get started with Cluster API Provider Nutanix Cloud Infrastructure (CAPX). To learn more about cluster API in more depth, check out the [Cluster API book](https://cluster-api.sigs.k8s.io/).

For more information on how install Nutanix CSI Driver on CAPX cluster, visit [Nutanix CSI Driver installation with CAPX](install_csi_driver.md)
For more information on how CAPX handles credentials, visit [Credential Management](./credential_management.md).

## Production workflow

### Build OS image for NutanixMachineTemplate resource
To build OS image for NutanixMachineTemplate, checkout [Nutanix OS Image builder](../tools/imagebuilder/README.md)

### Configure and Install Cluster API Provider Nutanix Cloud Infrastructure
To initialize Cluster API Provider Nutanix Cloud Infrastructure, clusterctl requires following variables, which should be set in either ~/.cluster-api/clusterctl.yaml or as environment variables
<pre>
NUTANIX_ENDPOINT: ""
NUTANIX_USER: ""
NUTANIX_PASSWORD: ""
NUTANIX_INSECURE: false # or true

KUBERNETES_VERSION: "v1.22.9"
WORKER_MACHINE_COUNT: 3
NUTANIX_SSH_AUTHORIZED_KEY: ""

NUTANIX_PRISM_ELEMENT_CLUSTER_NAME: ""
NUTANIX_MACHINE_TEMPLATE_IMAGE_NAME: ""
NUTANIX_SUBNET_NAME: ""
</pre>

you can also see the required list of variables by running by following command
<pre>
clusterctl generate cluster mycluster -i nutanix --list-variables           
Required Variables:
  - CONTROL_PLANE_ENDPOINT_IP
  - KUBERNETES_VERSION
  - NUTANIX_ENDPOINT
  - NUTANIX_MACHINE_TEMPLATE_IMAGE_NAME
  - NUTANIX_PASSWORD
  - NUTANIX_PRISM_ELEMENT_CLUSTER_NAME
  - NUTANIX_SSH_AUTHORIZED_KEY
  - NUTANIX_SUBNET_NAME
  - NUTANIX_USER

Optional Variables:
  - CONTROL_PLANE_ENDPOINT_PORT      (defaults to "6443")
  - CONTROL_PLANE_MACHINE_COUNT      (defaults to 1)
  - KUBEVIP_LB_ENABLE                (defaults to "false")
  - KUBEVIP_SVC_ENABLE               (defaults to "false")
  - NAMESPACE                        (defaults to current Namespace in the KubeConfig file)
  - NUTANIX_INSECURE                 (defaults to "false")
  - NUTANIX_MACHINE_BOOT_TYPE        (defaults to "legacy")
  - NUTANIX_MACHINE_MEMORY_SIZE      (defaults to "4Gi")
  - NUTANIX_MACHINE_VCPU_PER_SOCKET  (defaults to "1")
  - NUTANIX_MACHINE_VCPU_SOCKET      (defaults to "2")
  - NUTANIX_PORT                     (defaults to "9440")
  - NUTANIX_SYSTEMDISK_SIZE          (defaults to "40Gi")
  - WORKER_MACHINE_COUNT             (defaults to 0)

</pre>

Now you can instantiate Cluster API with the following
<pre>
clusterctl init -i nutanix
</pre>

### Deploy a workload Cluser on Nutanix Cloud Infrastructure
<pre>
export TEST_CLUSTER_NAME=mytestcluster1
export TEST_NAMESPACE=mytestnamespace
clusterctl generate cluster ${TEST_CLUSTER_NAME} \
    -i nutanix \
    --target-namespace ${TEST_NAMESPACE}  \
    --kubernetes-version v1.22.9 \
    --control-plane-machine-count 1 \
    --worker-machine-count 3 > ./cluster.yaml
kubectl create ns $(TEST_NAMESPACE)
kubectl apply -f ./cluster.yaml -n $(TEST_NAMESPACE)
</pre>

### Access workload Cluster
To access resources on the cluster, you can get the kubeconfig as following
<pre>
clusterctl get kubeconfig ${TEST_CLUSTER_NAME} -n ${TEST_NAMESPACE} > ${TEST_CLUSTER_NAME}.kubeconfig
kubectl --kubeconfig ./${TEST_CLUSTER_NAME}.kubeconfig get nodes 
</pre>

### Install CNI on workload Cluster

You must deploy a Container Network Interface (CNI) based Pod network add-on so that your Pods can communicate with each other. Cluster DNS (CoreDNS) will not start up before a network is installed.

Take care that your Pod network must not overlap with any of the host networks: you are likely to see problems if there is any overlap. (If you find a collision between your network plugin's preferred Pod network and some of your host networks, you should think of a suitable CIDR block to use instead, then use it inside the `cluster.yaml` generated by your `clusterctl generate cluster` before applying it)

Several external projects provide Kubernetes Pod networks using CNI, some of which also support [Network Policy](https://kubernetes.io/docs/concepts/services-networking/network-policies/).

See a list of add-ons that implement the [Kubernetes networking model](https://kubernetes.io/docs/concepts/cluster-administration/networking/#how-to-implement-the-kubernetes-networking-model). Most common are [Calico](https://www.tigera.io/project-calico/) and [Cilium](https://cilium.io).

You need to follow specific install guide for your selected CNI and you can install only one Pod network per cluster.

Once a Pod network has been installed, you can confirm that it is working by checking that the CoreDNS Pod is Running in the output of kubectl get pods --all-namespaces.


### kube-vip settings

kube-vip has the capability to provide a high availability address for both the Kubernetes control plane and for a Kubernetes Servic and also a true load balancing for the control plane to distribute API requests across control plane nodes.

You can tweak this settings by using the following properties

- KUBEVIP_LB_ENABLE

This setting allow you to enable control plane load balancing using IPVS.
[Control Plane Load-Balancing documentation](https://kube-vip.chipzoller.dev/docs/about/architecture/#control-plane-load-balancing)

- KUBEVIP_SVC_ENABLE 

This setting allow you to enable Service of type LoadBalancer.
[Kubernetes Service Load Balancing documentation](https://kube-vip.chipzoller.dev/docs/about/architecture/#kubernetes-service-load-balancing)


## Developer workflow

### Download source code
<pre>
git clone https://github.com/nutanix-cloud-native/cluster-api-provider-nutanix.git
cd cluster-api-provider-nutanix
</pre>

### Build source code
<pre>
make docker-build
</pre>

### Create test management cluster
<pre>
make kind-create
</pre>

### Deploy cluster-api-provider-nutanix CRDs on test management cluster
<pre>
make deploy
</pre>
### Deploy test workload cluster
Note: Update ./clusterctl.yaml with appropriate configuration before running following commands
<pre>
make prepare-local-clusterctl
make localtest
</pre>

### Delete test workfload cluster
<pre>
tbd
</pre>
