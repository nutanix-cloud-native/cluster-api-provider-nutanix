# NutanixMachineTemplate Controller: Autoscaling from 0 Enhancement

## Overview

The NutanixMachineTemplate controller has been enhanced to support **autoscaling from 0** by providing resource capacity information to the Kubernetes Cluster Autoscaler. This enables the autoscaler to make informed decisions about scaling up when no nodes are running.

## Understanding the Kubernetes Cluster Autoscaler

### **What is the Cluster Autoscaler?**



### **Core Functionality**

The Cluster Autoscaler operates on two main principles:

#### **Scale Up Decisions**
- **Trigger**: Pods are in "Pending" state due to insufficient node resources
- **Action**: Adds new nodes to the cluster
- **Logic**: Finds the most cost-effective node type that can accommodate pending pods

#### **Scale Down Decisions** 
- **Trigger**: Nodes are underutilized (typically < 50% resource usage)
- **Action**: Removes nodes from the cluster after safely evicting pods
- **Safety**: Ensures no disruption to running workloads

### **What Works Today (Regular Autoscaling)**

#### **Scale Up (1+ â†’ N nodes)**
When you already have nodes running, the autoscaler works perfectly:

```
Cluster State: [Node-1: 80% CPU] [Node-2: 75% CPU]
New Workload: Requires 4 CPUs
Decision: Add Node-3 (autoscaler can see existing node capacity)
Result: [Node-1: 80%] [Node-2: 75%] [Node-3: new workload]
```

**Why this works:**
- Existing nodes provide capacity information
- Autoscaler can calculate resource availability
- Clear signals for scaling decisions

#### **Scale Down (N â†’ 1+ nodes)**
Removing underutilized nodes also works well:

```
Cluster State: [Node-1: 20% CPU] [Node-2: 15% CPU] [Node-3: 10% CPU]
Decision: Remove Node-3 (safely drain and delete)
Result: [Node-1: 30% CPU] [Node-2: 25% CPU]
```

**Why this works:**
- Autoscaler can see actual node utilization
- Can safely evict pods to other nodes
- Clear metrics for scale-down decisions

## The Challenge: Scale To/From Zero

### **What Doesn't Work: Scale From Zero (0 â†’ 1+ nodes)**

```
Cluster State: [] (no nodes running)
New Workload: Pod requests cpu: 2, memory: 4Gi
Problem: How much capacity would a new node provide?
```

**The Fundamental Issue:**
- **No running nodes** = No capacity information available
- **Autoscaler is blind** = Can't determine if adding nodes would help
- **Workloads stuck** = Pods remain pending indefinitely

### **What's Challenging: Scale To Zero (1+ â†’ 0 nodes)**

```
Cluster State: [Node-1: 5% CPU] (very light usage)
Question: Should we remove the last node?
Problem: How will we scale back up when workloads arrive?
```

**The Catch-22:**
- **Remove last node** = Save costs but risk being unable to scale up
- **Keep last node** = Waste resources on idle infrastructure
- **Conservative approach** = Most autoscalers avoid going to absolute zero

### **Why Scale To/From Zero is Different**

| Aspect | Regular Autoscaling (1+ nodes) | Scale To/From Zero |
|--------|--------------------------------|-------------------|
| **Information Source** | Live node metrics & capacity | No live nodes to query |
| **Decision Confidence** | High (actual utilization data) | Low (no runtime information) |
| **Risk Level** | Low (incremental changes) | High (complete state change) |
| **Recovery Time** | Fast (nodes already exist) | Slow (full node provisioning) |
| **Cost Impact** | Predictable | Significant (idle costs vs boot time) |

## The Solution: Template-Based Capacity Advertisement

### **Breaking the Information Barrier**

The enhanced NutanixMachineTemplate controller solves the "scale from zero" problem by:

1. **Pre-calculating capacity** from VM specifications in the template
2. **Publishing capacity** in `Status.Capacity` field  
3. **Enabling informed decisions** even when no nodes are running

```yaml
# Template defines what WOULD be created
spec:
  template:
    spec:
      vcpusPerSocket: 4
      vcpuSockets: 2
      memorySize: 8Gi

# Controller calculates what capacity this WOULD provide  
status:
  capacity:
    cpu: "8"      # 4 Ã— 2 = 8 CPUs
    memory: 8Gi   # 8GB memory
```

### **Enabling Zero-Scale Workflows**

**Scale From Zero (0 â†’ 1+):**
```
Cluster State: [] (no nodes)
Pod Request: cpu: 2, memory: 4Gi
Template Check: NutanixMachineTemplate.status.capacity = {cpu: "8", memory: 8Gi}
Decision: Template can satisfy request â†’ Scale up!
Result: New node created with sufficient capacity
```

**Scale To Zero (1+ â†’ 0):**
```  
Cluster State: [Node-1: 2% CPU] (very low usage)
Template Available: Can scale back up when needed
Decision: Safe to remove last node â†’ Scale to zero!
Result: [] (no nodes) â†’ ðŸ’° Zero infrastructure costs
```

## Key Changes Made

### 1. **Resource Capacity Calculation**

The controller now calculates and publishes resource capacity information in the `NutanixMachineTemplate.Status.Capacity` field:

```go
// CPU Capacity: VCPUsPerSocket Ã— VCPUSockets
totalCPUs := machineSpec.VCPUsPerSocket * machineSpec.VCPUSockets
cpuQuantity := resource.NewQuantity(int64(totalCPUs), resource.DecimalSI)
nxMachineTemplate.Status.Capacity[infrav1.AutoscalerResourceCPU] = *cpuQuantity

// Memory Capacity: Direct from MemorySize
memoryQuantity := machineSpec.MemorySize.DeepCopy()
nxMachineTemplate.Status.Capacity[infrav1.AutoscalerResourceMemory] = memoryQuantity
```

### 2. **Enhanced RBAC Permissions**

Added comprehensive RBAC permissions to enable proper integration with Cluster API:

```yaml
//+kubebuilder:rbac:groups=infrastructure.cluster.x-k8s.io,resources=nutanixmachines,verbs=get;list;watch;create;update;patch;delete
//+kubebuilder:rbac:groups=infrastructure.cluster.x-k8s.io,resources=nutanixmachines/status,verbs=get;update;patch
//+kubebuilder:rbac:groups=infrastructure.cluster.x-k8s.io,resources=nutanixmachines/finalizers,verbs=update
//+kubebuilder:rbac:groups=cluster.x-k8s.io,resources=clusters,verbs=get;list;watch;update;patch
//+kubebuilder:rbac:groups=infrastructure.cluster.x-k8s.io,resources=nutanixclusters,verbs=get;list;watch;update;patch
//+kubebuilder:rbac:groups=bootstrap.cluster.x-k8s.io,resources=kubeadmconfigs,verbs=get;list;watch;update;patch
```

### 3. **Fixed Cluster-to-Template Mapping**

Corrected the `mapNutanixClusterToNutanixMachineTemplates()` function to properly map cluster changes to machine template reconciliation:

```go
// Before (incorrect):
if m.Spec.InfrastructureRef.Name == "" || m.Spec.InfrastructureRef.GroupVersionKind().Kind != "NutanixMachine" {
    continue
}
name := client.ObjectKey{Namespace: m.Namespace, Name: m.Spec.InfrastructureRef.Name}

// After (correct):
name := client.ObjectKey{Namespace: m.Namespace, Name: m.Name}
requests = append(requests, ctrl.Request{NamespacedName: name})
```

### 4. **Production-Ready Logging**

Streamlined logging to reduce noise while maintaining essential debugging information:
- **Error logs**: Always visible for troubleshooting
- **Info logs**: Only for important state changes (finalizer operations, deletions)
- **Debug logs**: `V(1)` level for detailed tracking when needed

## Real-World Impact: Cost vs Performance Trade-offs

### **Traditional Approach: Keep Minimum Nodes**
```
Cost Model: Always keep 1+ nodes running
ðŸ’° Monthly Cost: ~$200/month (idle nodes)
âš¡ Response Time: ~30 seconds (pod scheduling only)
ðŸ“Š Resource Waste: ~80% (nodes mostly idle)
```

### **Zero-Scale Approach: Scale to Absolute Zero** 
```
Cost Model: Scale to 0 when idle, scale up on demand
ðŸ’° Monthly Cost: ~$50/month (pay only when active)  
âš¡ Response Time: ~3-5 minutes (node provisioning + pod scheduling)
ðŸ“Š Resource Efficiency: ~95% (pay for what you use)
```

### **Use Cases Perfect for Zero-Scaling**

#### **Development/Test Clusters**
- **Pattern**: Active during work hours (8-10 hours/day)
- **Savings**: ~70% cost reduction
- **Impact**: Slight delay acceptable for non-production workloads

#### **Batch Processing Workloads**
- **Pattern**: Periodic jobs (nightly ETL, weekly reports)
- **Savings**: ~90% cost reduction  
- **Impact**: Job completion time matters more than start latency

#### **CI/CD Pipeline Agents**
- **Pattern**: Burst activity during code commits
- **Savings**: ~60% cost reduction
- **Impact**: Build times include node provisioning anyway

#### **Production User-Facing Applications**
- **Pattern**: Require sub-minute response times
- **Risk**: Customer experience impact
- **Alternative**: Use minimum node count with regular autoscaling

## How the Enhanced Controller Enables Zero-Scaling

### **Solving the Information Gap**

**Before Enhancement:**
```
Autoscaler Logic: "No nodes = no capacity info = can't scale up"
Result: Workloads stuck pending forever
```

**After Enhancement:**
```  
Autoscaler Logic: "Check template capacity = can satisfy request = scale up!"
Result: Intelligent scaling decisions even with zero nodes
```

#### **Resource Capacity Publishing**
```yaml
apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
kind: NutanixMachineTemplate
metadata:
  name: worker-template
spec:
  template:
    spec:
      vcpusPerSocket: 4      # 4 vCPUs per socket
      vcpuSockets: 2         # 2 sockets
      memorySize: 8Gi        # 8GB memory
status:
  capacity:
    cpu: "8"      # 4 Ã— 2 = 8 total CPUs
    memory: 8Gi   # 8GB memory
```

#### **Autoscaler Decision Process**
1. **Pod Pending**: A pod requests `cpu: 2, memory: 4Gi`
2. **No Nodes Available**: Cluster has 0 running nodes
3. **Capacity Check**: Autoscaler checks `NutanixMachineTemplate.Status.Capacity`
4. **Decision**: Template provides `cpu: 8, memory: 8Gi` â†’ **Scale Up!**
5. **Node Creation**: New NutanixMachine created with sufficient capacity

## Technical Implementation

### **Resource Calculation Logic**

```go
func (r *NutanixMachineTemplateReconciler) updateCapacity(ctx context.Context, nxMachineTemplate *infrav1.NutanixMachineTemplate) error {
    // Initialize capacity map
    if nxMachineTemplate.Status.Capacity == nil {
        nxMachineTemplate.Status.Capacity = make(corev1.ResourceList)
    }

    // Extract VM specifications
    machineSpec := nxMachineTemplate.Spec.Template.Spec

    // Calculate total CPU: sockets Ã— cores per socket
    totalCPUs := machineSpec.VCPUsPerSocket * machineSpec.VCPUSockets
    cpuQuantity := resource.NewQuantity(int64(totalCPUs), resource.DecimalSI)
    nxMachineTemplate.Status.Capacity[infrav1.AutoscalerResourceCPU] = *cpuQuantity

    // Set memory capacity directly from spec
    memoryQuantity := machineSpec.MemorySize.DeepCopy()
    nxMachineTemplate.Status.Capacity[infrav1.AutoscalerResourceMemory] = memoryQuantity

    return nil
}
```

### **Integration with Cluster API**

The controller watches multiple resource types to ensure capacity information stays current:
- **NutanixMachineTemplate**: Direct reconciliation
- **NutanixCluster**: Updates when cluster infrastructure changes
- **Cluster**: Updates when CAPI cluster state changes
- **Machine**: Updates when individual machines change

## Benefits

### **Zero-Downtime Scaling**
- Cluster can scale from 0 nodes to multiple nodes automatically
- No manual intervention required when workloads need resources

### **Cost Optimization**
- Clusters can scale down to 0 when idle
- Automatic scale-up when workloads arrive
- Pay only for resources when needed

### **Resource Awareness**
- Autoscaler makes informed decisions based on actual VM specifications
- Prevents over-provisioning or under-provisioning scenarios

### **Production Ready**
- Clean, efficient logging
- Proper error handling and retry logic
- Comprehensive RBAC permissions

## Example Usage

### **MachineTemplate Definition**
```yaml
apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
kind: NutanixMachineTemplate
metadata:
  name: high-memory-template
spec:
  template:
    spec:
      vcpusPerSocket: 8       # 8 cores per socket
      vcpuSockets: 1          # 1 socket
      memorySize: 32Gi        # 32GB RAM
      systemDiskSize: 100Gi
      # ... other VM specs
```

### **Resulting Capacity**
```yaml
status:
  capacity:
    cpu: "8"        # 8 Ã— 1 = 8 CPUs
    memory: 32Gi    # 32GB memory
```

### **Autoscaler Behavior**
- **Pending Pod**: Requires `cpu: 4, memory: 16Gi`
- **Template Check**: `high-memory-template` provides `cpu: 8, memory: 32Gi`
- **Result**: Template has sufficient capacity â†’ Scale up triggered
- **Outcome**: New node created with 8 CPUs and 32GB RAM

## Monitoring

### **Check Capacity Status**
```bash
kubectl get nutanixmachinetemplates -o jsonpath='{.items[*].status.capacity}'
```

### **Watch Scaling Events**
```bash
kubectl get events --field-selector reason=TriggeredScaleUp
```

This enhancement transforms the NutanixMachineTemplate controller into a key component for efficient, cost-effective autoscaling in Nutanix-based Kubernetes clusters! ðŸš€
