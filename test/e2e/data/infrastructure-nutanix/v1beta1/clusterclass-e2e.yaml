apiVersion: v1
binaryData:
  ca.crt: ${NUTANIX_ADDITIONAL_TRUST_BUNDLE=""}
kind: ConfigMap
metadata:
  name: ${CLUSTER_NAME}-pc-trusted-ca-bundle
  namespace: ${NAMESPACE}
---
apiVersion: v1
data:
  nutanix-ccm.yaml: |
    # Source: nutanix-cloud-provider/templates/rbac.yaml
    apiVersion: v1
    kind: ServiceAccount
    metadata:
      name: cloud-controller-manager
      namespace: kube-system
    ---
    # Source: nutanix-cloud-provider/templates/cm.yaml
    kind: ConfigMap
    apiVersion: v1
    metadata:
      name: nutanix-config
      namespace: kube-system
    data:
      nutanix_config.json: |-
        {
          "prismCentral": {
            "address": "${NUTANIX_ENDPOINT}",
            "port": ${NUTANIX_PORT=9440},
            "insecure": ${NUTANIX_INSECURE=false},
            "credentialRef": {
              "kind": "secret",
              "name": "nutanix-creds",
              "namespace": "kube-system"
            },
            "additionalTrustBundle": {
              "kind": "ConfigMap",
              "name": "${CLUSTER_NAME}-pc-trusted-ca-bundle",
              "namespace": "kube-system"
            }
          },
          "enableCustomLabeling": ${CCM_CUSTOM_LABEL=false},
          "topologyDiscovery": {
            "type": "Prism"
          }
        }
    ---
    # Source: nutanix-cloud-provider/templates/rbac.yaml
    apiVersion: rbac.authorization.k8s.io/v1
    kind: ClusterRole
    metadata:
      annotations:
        rbac.authorization.kubernetes.io/autoupdate: "true"
      name: system:cloud-controller-manager
    rules:
      - apiGroups:
          - ""
        resources:
          - secrets
        verbs:
          - get
          - list
          - watch
      - apiGroups:
          - ""
        resources:
          - configmaps
        verbs:
          - get
          - list
          - watch
      - apiGroups:
          - ""
        resources:
          - events
        verbs:
          - create
          - patch
          - update
      - apiGroups:
          - ""
        resources:
          - nodes
        verbs:
          - "*"
      - apiGroups:
          - ""
        resources:
          - nodes/status
        verbs:
          - patch
      - apiGroups:
          - ""
        resources:
          - serviceaccounts
        verbs:
          - create
      - apiGroups:
          - ""
        resources:
          - endpoints
        verbs:
          - create
          - get
          - list
          - watch
          - update
      - apiGroups:
          - coordination.k8s.io
        resources:
          - leases
        verbs:
          - get
          - list
          - watch
          - create
          - update
          - patch
          - delete
    ---
    # Source: nutanix-cloud-provider/templates/rbac.yaml
    kind: ClusterRoleBinding
    apiVersion: rbac.authorization.k8s.io/v1
    metadata:
      name: system:cloud-controller-manager
    roleRef:
      apiGroup: rbac.authorization.k8s.io
      kind: ClusterRole
      name: system:cloud-controller-manager
    subjects:
      - kind: ServiceAccount
        name: cloud-controller-manager
        namespace: kube-system
    ---
    # Source: nutanix-cloud-provider/templates/cloud-provider-nutanix-deployment.yaml
    apiVersion: apps/v1
    kind: Deployment
    metadata:
      labels:
        k8s-app: nutanix-cloud-controller-manager
      name: nutanix-cloud-controller-manager
      namespace: kube-system
    spec:
      replicas: 1
      selector:
        matchLabels:
          k8s-app: nutanix-cloud-controller-manager
      strategy:
        type: Recreate
      template:
        metadata:
          labels:
            k8s-app: nutanix-cloud-controller-manager
        spec:
          hostNetwork: true
          priorityClassName: system-cluster-critical
          nodeSelector:
            node-role.kubernetes.io/control-plane: ""
          serviceAccountName: cloud-controller-manager
          affinity:
            podAntiAffinity:
              requiredDuringSchedulingIgnoredDuringExecution:
              - labelSelector:
                  matchLabels:
                    k8s-app: nutanix-cloud-controller-manager
                topologyKey: kubernetes.io/hostname
          dnsPolicy: Default
          tolerations:
            - effect: NoSchedule
              key: node-role.kubernetes.io/master
              operator: Exists
            - effect: NoSchedule
              key: node-role.kubernetes.io/control-plane
              operator: Exists
            - effect: NoExecute
              key: node.kubernetes.io/unreachable
              operator: Exists
              tolerationSeconds: 120
            - effect: NoExecute
              key: node.kubernetes.io/not-ready
              operator: Exists
              tolerationSeconds: 120
            - effect: NoSchedule
              key: node.cloudprovider.kubernetes.io/uninitialized
              operator: Exists
            - effect: NoSchedule
              key: node.kubernetes.io/not-ready
              operator: Exists
          containers:
            - image: "${CCM_REPO=ghcr.io/nutanix-cloud-native/cloud-provider-nutanix/controller}:${CCM_TAG=v0.3.1}"
              imagePullPolicy: IfNotPresent
              name: nutanix-cloud-controller-manager
              env:
                - name: POD_NAMESPACE
                  valueFrom:
                    fieldRef:
                      fieldPath: metadata.namespace
              args:
                - "--leader-elect=true"
                - "--cloud-config=/etc/cloud/nutanix_config.json"
              resources:
                requests:
                  cpu: 100m
                  memory: 50Mi
              volumeMounts:
                - mountPath: /etc/cloud
                  name: nutanix-config-volume
                  readOnly: true
          volumes:
            - name: nutanix-config-volume
              configMap:
                name: nutanix-config
kind: ConfigMap
metadata:
  name: nutanix-ccm
  namespace: ${NAMESPACE}
---
apiVersion: v1
kind: Secret
metadata:
  name: ${CLUSTER_NAME}
  namespace: ${NAMESPACE}
stringData:
  credentials: "[\n  {\n    \"type\": \"basic_auth\", \n    \"data\": { \n      \"prismCentral\":{\n
    \       \"username\": \"${NUTANIX_USER}\", \n        \"password\": \"${NUTANIX_PASSWORD}\"\n
    \     }\n    }\n  }\n]\n"
---
apiVersion: v1
kind: Secret
metadata:
  name: nutanix-ccm-secret
  namespace: ${NAMESPACE}
stringData:
  nutanix-ccm-secret.yaml: "apiVersion: v1\nkind: Secret\nmetadata:\n  name: nutanix-creds\n
    \ namespace: kube-system\nstringData:\n  credentials: |\n    [\n      {\n        \"type\":
    \"basic_auth\", \n        \"data\": { \n          \"prismCentral\":{\n            \"username\":
    \"${NUTANIX_USER}\",\n            \"password\": \"${NUTANIX_PASSWORD}\"\n          },\n
    \         \"prismElements\": null\n        }\n      }\n    ]\n"
type: addons.cluster.x-k8s.io/resource-set
---
apiVersion: addons.cluster.x-k8s.io/v1beta1
kind: ClusterResourceSet
metadata:
  name: nutanix-ccm-crs
  namespace: ${NAMESPACE}
spec:
  clusterSelector:
    matchLabels:
      ccm: nutanix
  resources:
  - kind: ConfigMap
    name: nutanix-ccm
  - kind: Secret
    name: nutanix-ccm-secret
  - kind: ConfigMap
    name: ${CLUSTER_NAME}-pc-trusted-ca-bundle
  strategy: ApplyOnce
---
apiVersion: bootstrap.cluster.x-k8s.io/v1beta1
kind: KubeadmConfigTemplate
metadata:
  name: ${CLUSTER_NAME}-kcfg-0
  namespace: ${NAMESPACE}
spec:
  template:
    spec:
      joinConfiguration:
        nodeRegistration:
          kubeletExtraArgs:
            cloud-provider: external
            eviction-hard: nodefs.available<10%,nodefs.inodesFree<5%,imagefs.available<15%,memory.available<100Mi,imagefs.inodesFree<10%
            tls-cipher-suites: ${TLS_CIPHER_SUITES=TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305_SHA256,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305_SHA256}
      postKubeadmCommands:
      - echo "after kubeadm call" > /var/log/postkubeadm.log
      preKubeadmCommands:
      - echo "before kubeadm call" > /var/log/prekubeadm.log
      - hostnamectl set-hostname "{{ ds.meta_data.hostname }}"
      verbosity: 10
---
apiVersion: cluster.x-k8s.io/v1beta1
kind: Cluster
metadata:
  labels:
    ccm: nutanix
    cluster.x-k8s.io/cluster-name: ${CLUSTER_NAME}
    cni: ${CLUSTER_NAME}-crs-cni
  name: ${CLUSTER_NAME}
  namespace: ${NAMESPACE}
spec:
  clusterNetwork:
    pods:
      cidrBlocks:
      - 172.20.0.0/16
    serviceDomain: cluster.local
    services:
      cidrBlocks:
      - 172.19.0.0/16
  topology:
    class: ${CLUSTER_CLASS_NAME}
    controlPlane:
      metadata: {}
      replicas: ${CONTROL_PLANE_MACHINE_COUNT}
    variables:
    - name: sshKey
      value: ${NUTANIX_SSH_AUTHORIZED_KEY}
    version: ${KUBERNETES_VERSION}
    workers:
      machineDeployments:
      - class: ${CLUSTER_CLASS_NAME}-worker
        metadata: {}
        name: md-0
        replicas: ${WORKER_MACHINE_COUNT}
---
apiVersion: cluster.x-k8s.io/v1beta1
kind: ClusterClass
metadata:
  name: ${CLUSTER_CLASS_NAME}
  namespace: ${NAMESPACE}
spec:
  controlPlane:
    machineHealthCheck:
      maxUnhealthy: 40%
      nodeStartupTimeout: 10m
      unhealthyConditions:
      - status: "False"
        timeout: 300s
        type: Ready
      - status: Unknown
        timeout: 300s
        type: Ready
      - status: "True"
        timeout: 300s
        type: MemoryPressure
      - status: "True"
        timeout: 300s
        type: DiskPressure
      - status: "True"
        timeout: 300s
        type: PIDPressure
      - status: "True"
        timeout: 300s
        type: NetworkUnavailable
    machineInfrastructure:
      ref:
        apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
        kind: NutanixMachineTemplate
        name: ${CLUSTER_CLASS_NAME}-cp-nmt
        namespace: ${NAMESPACE}
    ref:
      apiVersion: controlplane.cluster.x-k8s.io/v1beta1
      kind: KubeadmControlPlaneTemplate
      name: ${CLUSTER_CLASS_NAME}-kcpt
      namespace: ${NAMESPACE}
  infrastructure:
    ref:
      apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
      kind: NutanixClusterTemplate
      name: ${CLUSTER_CLASS_NAME}-nct
      namespace: ${NAMESPACE}
  patches:
  - definitions:
    - jsonPatches:
      - op: add
        path: /spec/template/spec/kubeadmConfigSpec/users
        valueFrom:
          template: |
            - name: capxuser
              lockPassword: false
              sudo: ALL=(ALL) NOPASSWD:ALL
              sshAuthorizedKeys:
                - '{{ .sshKey }}'
      selector:
        apiVersion: controlplane.cluster.x-k8s.io/v1beta1
        kind: KubeadmControlPlaneTemplate
        matchResources:
          controlPlane: true
    - jsonPatches:
      - op: add
        path: /spec/template/spec/users
        valueFrom:
          template: |
            - name: capxuser
              lockPassword: false
              sudo: ALL=(ALL) NOPASSWD:ALL
              sshAuthorizedKeys:
                - '{{ .sshKey }}'
      selector:
        apiVersion: bootstrap.cluster.x-k8s.io/v1beta1
        kind: KubeadmConfigTemplate
        matchResources:
          machineDeploymentClass:
            names:
            - ${CLUSTER_CLASS_NAME}-worker
    name: add-ssh-user
  variables:
  - name: sshKey
    required: false
    schema:
      openAPIV3Schema:
        description: Public key to SSH onto the cluster nodes.
        type: string
  workers:
    machineDeployments:
    - class: ${CLUSTER_CLASS_NAME}-worker
      machineHealthCheck:
        maxUnhealthy: 40%
        nodeStartupTimeout: 10m
        unhealthyConditions:
        - status: "False"
          timeout: 300s
          type: Ready
        - status: Unknown
          timeout: 300s
          type: Ready
        - status: "True"
          timeout: 300s
          type: MemoryPressure
        - status: "True"
          timeout: 300s
          type: DiskPressure
        - status: "True"
          timeout: 300s
          type: PIDPressure
        - status: "True"
          timeout: 300s
          type: NetworkUnavailable
      template:
        bootstrap:
          ref:
            apiVersion: bootstrap.cluster.x-k8s.io/v1beta1
            kind: KubeadmConfigTemplate
            name: ${CLUSTER_NAME}-kcfg-0
            namespace: ${NAMESPACE}
        infrastructure:
          ref:
            apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
            kind: NutanixMachineTemplate
            name: ${CLUSTER_CLASS_NAME}-md-nmt
            namespace: ${NAMESPACE}
---
apiVersion: controlplane.cluster.x-k8s.io/v1beta1
kind: KubeadmControlPlaneTemplate
metadata:
  name: ${CLUSTER_CLASS_NAME}-kcpt
  namespace: ${NAMESPACE}
spec:
  template:
    spec:
      kubeadmConfigSpec:
        clusterConfiguration:
          apiServer:
            certSANs:
            - localhost
            - 127.0.0.1
            - 0.0.0.0
            extraArgs:
              cloud-provider: external
              tls-cipher-suites: ${TLS_CIPHER_SUITES=TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305_SHA256,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305_SHA256}
          controllerManager:
            extraArgs:
              cloud-provider: external
              enable-hostpath-provisioner: "true"
              tls-cipher-suites: ${TLS_CIPHER_SUITES=TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305_SHA256,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305_SHA256}
          scheduler:
            extraArgs:
              tls-cipher-suites: ${TLS_CIPHER_SUITES=TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305_SHA256,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305_SHA256}
        files:
        - content: |
            apiVersion: v1
            kind: Pod
            metadata:
              name: kube-vip
              namespace: kube-system
            spec:
              containers:
                - name: kube-vip
                  image: ghcr.io/kube-vip/kube-vip:v0.6.4
                  imagePullPolicy: IfNotPresent
                  args:
                    - manager
                  env:
                    - name: vip_arp
                      value: "true"
                    - name: address
                      value: "${CONTROL_PLANE_ENDPOINT_IP}"
                    - name: port
                      value: "${CONTROL_PLANE_ENDPOINT_PORT=6443}"
                    - name: vip_cidr
                      value: "32"
                    - name: cp_enable
                      value: "true"
                    - name: cp_namespace
                      value: kube-system
                    - name: vip_ddns
                      value: "false"
                    - name: vip_leaderelection
                      value: "true"
                    - name: vip_leaseduration
                      value: "15"
                    - name: vip_renewdeadline
                      value: "10"
                    - name: vip_retryperiod
                      value: "2"
                    - name: svc_enable
                      value: "${KUBEVIP_SVC_ENABLE=false}"
                    - name: lb_enable
                      value: "${KUBEVIP_LB_ENABLE=false}"
                    - name: enableServicesElection
                      value: "${KUBEVIP_SVC_ELECTION=false}"
                  securityContext:
                    capabilities:
                      add:
                        - NET_ADMIN
                        - SYS_TIME
                        - NET_RAW
                  volumeMounts:
                    - mountPath: /etc/kubernetes/admin.conf
                      name: kubeconfig
                  resources: {}
              hostNetwork: true
              hostAliases:
                - hostnames:
                    - kubernetes
                  ip: 127.0.0.1
              volumes:
                - name: kubeconfig
                  hostPath:
                    type: FileOrCreate
                    path: /etc/kubernetes/admin.conf
            status: {}
          owner: root:root
          path: /etc/kubernetes/manifests/kube-vip.yaml
        initConfiguration:
          nodeRegistration:
            kubeletExtraArgs:
              cloud-provider: external
              eviction-hard: nodefs.available<10%,nodefs.inodesFree<5%,imagefs.available<15%,memory.available<100Mi,imagefs.inodesFree<10%
              tls-cipher-suites: ${TLS_CIPHER_SUITES=TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305_SHA256,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305_SHA256}
        joinConfiguration:
          nodeRegistration:
            kubeletExtraArgs:
              cloud-provider: external
              eviction-hard: nodefs.available<10%,nodefs.inodesFree<5%,imagefs.available<15%,memory.available<100Mi,imagefs.inodesFree<10%
              tls-cipher-suites: ${TLS_CIPHER_SUITES=TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305_SHA256,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305_SHA256}
        postKubeadmCommands:
        - echo export KUBECONFIG=/etc/kubernetes/admin.conf >> /root/.bashrc
        - echo "after kubeadm call" > /var/log/postkubeadm.log
        preKubeadmCommands:
        - echo "before kubeadm call" > /var/log/prekubeadm.log
        - hostnamectl set-hostname "{{ ds.meta_data.hostname }}"
        useExperimentalRetryJoin: true
        verbosity: 10
---
apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
kind: NutanixClusterTemplate
metadata:
  name: ${CLUSTER_CLASS_NAME}-nct
  namespace: ${NAMESPACE}
spec:
  template:
    spec:
      controlPlaneEndpoint:
        host: ${CONTROL_PLANE_ENDPOINT_IP}
        port: ${CONTROL_PLANE_ENDPOINT_PORT=6443}
      failureDomains: []
      prismCentral:
        additionalTrustBundle:
          kind: ConfigMap
          name: ${CLUSTER_NAME}-pc-trusted-ca-bundle
        address: ${NUTANIX_ENDPOINT}
        credentialRef:
          kind: Secret
          name: ${CLUSTER_NAME}
        insecure: ${NUTANIX_INSECURE=false}
        port: ${NUTANIX_PORT=9440}
---
apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
kind: NutanixMachineTemplate
metadata:
  name: ${CLUSTER_CLASS_NAME}-cp-nmt
  namespace: ${NAMESPACE}
spec:
  template:
    spec:
      bootType: ${NUTANIX_MACHINE_BOOT_TYPE=legacy}
      cluster:
        name: ${NUTANIX_PRISM_ELEMENT_CLUSTER_NAME}
        type: name
      image:
        name: ${NUTANIX_MACHINE_TEMPLATE_IMAGE_NAME}
        type: name
      memorySize: ${NUTANIX_MACHINE_MEMORY_SIZE=4Gi}
      providerID: nutanix://${CLUSTER_NAME}-m1
      subnet:
      - name: ${NUTANIX_SUBNET_NAME}
        type: name
      systemDiskSize: ${NUTANIX_SYSTEMDISK_SIZE=40Gi}
      vcpuSockets: ${NUTANIX_MACHINE_VCPU_SOCKET=2}
      vcpusPerSocket: ${NUTANIX_MACHINE_VCPU_PER_SOCKET=1}
---
apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
kind: NutanixMachineTemplate
metadata:
  name: ${CLUSTER_CLASS_NAME}-md-nmt
  namespace: ${NAMESPACE}
spec:
  template:
    spec:
      bootType: ${NUTANIX_MACHINE_BOOT_TYPE=legacy}
      cluster:
        name: ${NUTANIX_PRISM_ELEMENT_CLUSTER_NAME}
        type: name
      image:
        name: ${NUTANIX_MACHINE_TEMPLATE_IMAGE_NAME}
        type: name
      memorySize: ${NUTANIX_MACHINE_MEMORY_SIZE=4Gi}
      providerID: nutanix://${CLUSTER_NAME}-m1
      subnet:
      - name: ${NUTANIX_SUBNET_NAME}
        type: name
      systemDiskSize: ${NUTANIX_SYSTEMDISK_SIZE=40Gi}
      vcpuSockets: ${NUTANIX_MACHINE_VCPU_SOCKET=2}
      vcpusPerSocket: ${NUTANIX_MACHINE_VCPU_PER_SOCKET=1}
